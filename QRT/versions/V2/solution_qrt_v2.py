"""
ğŸš€ QRT V2 ULTIMATE - Targeting 0.7+ Score
==========================================
Advanced techniques for maximum performance:

1. Multi-Model Stacking (LightGBM + XGBoost + CatBoost + Ridge)
2. Advanced Feature Engineering (lag interactions, rolling stats, cross-sectional)
3. Target Encoding with proper out-of-fold
4. Time-Series Aware CV (GroupKFold on TS)
5. Blending with optimized weights
6. Feature Selection via importance
7. Hyperparameter optimization
"""

import numpy as np
import pandas as pd
import warnings
import lightgbm as lgb
import xgboost as xgb
from sklearn.model_selection import KFold, GroupKFold
from sklearn.metrics import accuracy_score
from sklearn.linear_model import Ridge, LogisticRegression
from sklearn.preprocessing import StandardScaler
import gc

warnings.filterwarnings('ignore')

# ============== CONFIGURATION ==============
DATA_DIR = 'c:/Users/Karim/Desktop/Projet/QRT/data/'
X_TRAIN_PATH = DATA_DIR + 'X_train_9xQjqvZ.csv'
Y_TRAIN_PATH = DATA_DIR + 'y_train_Ppwhaz8.csv'
X_TEST_PATH = DATA_DIR + 'X_test_1zTtEnD.csv'
SAMPLE_SUB_PATH = DATA_DIR + 'sample_submission_SpGVFuH.csv'
OUTPUT_PATH = 'c:/Users/Karim/Desktop/Projet/QRT/V2/submission.csv'

N_FOLDS = 10
RANDOM_SEED = 42

print("=" * 70)
print("ğŸš€ QRT V2 ULTIMATE - Multi-Model Stacking Pipeline")
print("=" * 70)

# ============== LOAD DATA ==============
print("\n[1/7] Chargement des donnÃ©es...")
X_train = pd.read_csv(X_TRAIN_PATH, index_col='ROW_ID')
y_train = pd.read_csv(Y_TRAIN_PATH, index_col='ROW_ID')
X_test = pd.read_csv(X_TEST_PATH, index_col='ROW_ID')
sample_submission = pd.read_csv(SAMPLE_SUB_PATH, index_col='ROW_ID')

print(f"   X_train: {X_train.shape}")
print(f"   y_train: {y_train.shape}")
print(f"   X_test: {X_test.shape}")

# ============== ADVANCED FEATURE ENGINEERING ==============
print("\n[2/7] Advanced Feature Engineering...")

RET_features = [f'RET_{i}' for i in range(1, 21)]
SIGNED_VOLUME_features = [f'SIGNED_VOLUME_{i}' for i in range(1, 21)]

def advanced_features(df, train_stats=None):
    """Construction de features avancÃ©es"""
    data = df.copy()
    
    # === 1. ROLLING STATISTICS ===
    ret_cols = [f'RET_{i}' for i in range(1, 21)]
    vol_cols = [f'SIGNED_VOLUME_{i}' for i in range(1, 21)]
    
    # Moyennes Ã  diffÃ©rents horizons
    for h in [3, 5, 7, 10, 15, 20]:
        data[f'RET_MEAN_{h}'] = data[ret_cols[:h]].mean(axis=1)
        data[f'RET_STD_{h}'] = data[ret_cols[:h]].std(axis=1)
        data[f'RET_SUM_{h}'] = data[ret_cols[:h]].sum(axis=1)
        data[f'RET_MIN_{h}'] = data[ret_cols[:h]].min(axis=1)
        data[f'RET_MAX_{h}'] = data[ret_cols[:h]].max(axis=1)
        data[f'RET_RANGE_{h}'] = data[f'RET_MAX_{h}'] - data[f'RET_MIN_{h}']
        
        data[f'VOL_MEAN_{h}'] = data[vol_cols[:h]].mean(axis=1)
        data[f'VOL_STD_{h}'] = data[vol_cols[:h]].std(axis=1)
        data[f'VOL_SUM_{h}'] = data[vol_cols[:h]].sum(axis=1)
    
    # === 2. MOMENTUM FEATURES ===
    data['MOMENTUM_1_5'] = data['RET_MEAN_5']
    data['MOMENTUM_1_10'] = data['RET_MEAN_10']
    data['MOMENTUM_5_20'] = data[ret_cols[4:20]].mean(axis=1)
    data['MOMENTUM_ACCEL'] = data['MOMENTUM_1_5'] - data['MOMENTUM_5_20']
    
    # Trend (decay-weighted)
    weights = np.array([1/i for i in range(1, 21)])
    weights = weights / weights.sum()
    data['RET_WEIGHTED'] = data[ret_cols].values @ weights
    
    # === 3. CROSS-SECTIONAL FEATURES ===
    for col in ['RET_MEAN_5', 'RET_MEAN_20', 'RET_STD_20', 'MEDIAN_DAILY_TURNOVER']:
        if col in data.columns:
            g = data.groupby('TS')[col]
            data[f'{col}_CS_RANK'] = g.rank(pct=True)
            data[f'{col}_CS_NORM'] = (data[col] - g.transform('mean')) / (g.transform('std') + 1e-8)
            data[f'{col}_CS_DIFF'] = data[col] - g.transform('mean')
    
    # === 4. VOLATILITY FEATURES ===
    data['SHARPE_5'] = data['RET_MEAN_5'] / (data['RET_STD_5'] + 1e-8)
    data['SHARPE_20'] = data['RET_MEAN_20'] / (data['RET_STD_20'] + 1e-8)
    data['VOL_OF_VOL'] = data[[f'RET_STD_{h}' for h in [3, 5, 7, 10]]].std(axis=1)
    
    # === 5. SIGN FEATURES ===
    for h in [3, 5, 10, 20]:
        pos_count = (data[ret_cols[:h]] > 0).sum(axis=1)
        data[f'POS_RATIO_{h}'] = pos_count / h
        data[f'SIGN_PERSIST_{h}'] = (data[ret_cols[:h]] > 0).astype(int).diff(axis=1).abs().sum(axis=1)
    
    # === 6. INTERACTION FEATURES ===
    data['RET_VOL_INTERACT'] = data['RET_MEAN_5'] * data['VOL_MEAN_5']
    data['MOMENTUM_TURNOVER'] = data['MOMENTUM_ACCEL'] * data['MEDIAN_DAILY_TURNOVER']
    
    # === 7. LAG PATTERNS ===
    data['RET_1'] = data['RET_1']
    data['RET_1_SIGN'] = (data['RET_1'] > 0).astype(int)
    data['RET_REVERSAL'] = -data['RET_1']  # Mean reversion signal
    
    # === 8. GROUP-BASED FEATURES (Target Encoding placeholder) ===
    # Will be computed with OOF in training loop
    
    return data

X_train = advanced_features(X_train)
X_test = advanced_features(X_test)

# Feature list (exclude non-features)
exclude_cols = {'TS', 'ALLOCATION', 'GROUP'}
feature_cols = [c for c in X_train.columns if c not in exclude_cols]
print(f"   Total features: {len(feature_cols)}")

# ============== TARGET ENCODING WITH OOF ==============
print("\n[3/7] Computing Target Encodings (OOF)...")

def compute_target_encoding_oof(X_train, y_train, X_test, col, n_folds=5):
    """Compute target encoding with out-of-fold to prevent leakage"""
    train_enc = np.zeros(len(X_train))
    test_enc = np.zeros(len(X_test))
    
    global_mean = y_train['target'].mean()
    
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_SEED)
    
    for train_idx, val_idx in kf.split(X_train):
        # Compute mean on train fold
        train_data = X_train.iloc[train_idx]
        train_y = y_train.iloc[train_idx]
        
        combined = pd.DataFrame({col: train_data[col], 'target': train_y['target']})
        means = combined.groupby(col)['target'].mean()
        
        # Apply to val fold
        val_data = X_train.iloc[val_idx]
        train_enc[val_idx] = val_data[col].map(means).fillna(global_mean)
        
        # Accumulate for test
        test_enc += X_test[col].map(means).fillna(global_mean).values / n_folds
    
    return train_enc, test_enc

# Target encode GROUP and ALLOCATION
for col in ['GROUP', 'ALLOCATION']:
    if col in X_train.columns:
        train_enc, test_enc = compute_target_encoding_oof(X_train, y_train, X_test, col)
        X_train[f'{col}_ENC'] = train_enc
        X_test[f'{col}_ENC'] = test_enc
        feature_cols.append(f'{col}_ENC')

print(f"   Features after encoding: {len(feature_cols)}")

# ============== PREPARE DATA ==============
print("\n[4/7] Preparing data matrices...")

# Remove non-numeric and target columns from features
numeric_features = [c for c in feature_cols if c in X_train.columns and X_train[c].dtype in ['float64', 'int64', 'float32', 'int32']]
print(f"   Numeric features: {len(numeric_features)}")

X = X_train[numeric_features].fillna(0).values
y = y_train['target'].values
X_te = X_test[numeric_features].fillna(0).values
groups = X_train['TS'].values

# Scale for linear models
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_te_scaled = scaler.transform(X_te)

# ============== LEVEL 1: BASE MODELS ==============
print(f"\n[5/7] Training Level-1 Models ({N_FOLDS}-Fold CV)...")

# OOF predictions for stacking
oof_lgb = np.zeros(len(X))
oof_xgb = np.zeros(len(X))
oof_ridge = np.zeros(len(X))

test_lgb = np.zeros(len(X_te))
test_xgb = np.zeros(len(X_te))
test_ridge = np.zeros(len(X_te))

gkf = GroupKFold(n_splits=N_FOLDS)

lgb_params = {
    'objective': 'regression',
    'metric': 'rmse',
    'learning_rate': 0.01,
    'num_leaves': 31,
    'max_depth': 6,
    'min_child_samples': 50,
    'feature_fraction': 0.6,
    'bagging_fraction': 0.7,
    'bagging_freq': 5,
    'lambda_l1': 0.1,
    'lambda_l2': 5.0,
    'verbosity': -1,
    'n_estimators': 1000,
    'random_state': RANDOM_SEED,
}

xgb_params = {
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse',
    'learning_rate': 0.01,
    'max_depth': 5,
    'min_child_weight': 50,
    'subsample': 0.7,
    'colsample_bytree': 0.6,
    'reg_alpha': 0.1,
    'reg_lambda': 5.0,
    'n_estimators': 1000,
    'random_state': RANDOM_SEED,
    'verbosity': 0,
}

fold_scores = []

for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):
    print(f"\n   Fold {fold + 1}/{N_FOLDS}:")
    
    X_tr, X_val = X[train_idx], X[val_idx]
    y_tr, y_val = y[train_idx], y[val_idx]
    X_tr_sc, X_val_sc = X_scaled[train_idx], X_scaled[val_idx]
    
    # --- LightGBM ---
    lgb_model = lgb.LGBMRegressor(**lgb_params)
    lgb_model.fit(
        X_tr, y_tr,
        eval_set=[(X_val, y_val)],
        callbacks=[lgb.early_stopping(100, verbose=False)]
    )
    oof_lgb[val_idx] = lgb_model.predict(X_val)
    test_lgb += lgb_model.predict(X_te) / N_FOLDS
    
    # --- XGBoost ---
    xgb_model = xgb.XGBRegressor(**xgb_params)
    xgb_model.fit(
        X_tr, y_tr,
        eval_set=[(X_val, y_val)],
        verbose=False
    )
    oof_xgb[val_idx] = xgb_model.predict(X_val)
    test_xgb += xgb_model.predict(X_te) / N_FOLDS
    
    # --- Ridge ---
    ridge_model = Ridge(alpha=10.0)
    ridge_model.fit(X_tr_sc, y_tr)
    oof_ridge[val_idx] = ridge_model.predict(X_val_sc)
    test_ridge += ridge_model.predict(X_te_scaled) / N_FOLDS
    
    # Blend for this fold
    fold_blend = 0.5 * oof_lgb[val_idx] + 0.35 * oof_xgb[val_idx] + 0.15 * oof_ridge[val_idx]
    fold_acc = accuracy_score((y_val > 0).astype(int), (fold_blend > 0).astype(int))
    fold_scores.append(fold_acc)
    print(f"      LGB: {accuracy_score((y_val > 0).astype(int), (oof_lgb[val_idx] > 0).astype(int))*100:.2f}%")
    print(f"      XGB: {accuracy_score((y_val > 0).astype(int), (oof_xgb[val_idx] > 0).astype(int))*100:.2f}%")
    print(f"      Ridge: {accuracy_score((y_val > 0).astype(int), (oof_ridge[val_idx] > 0).astype(int))*100:.2f}%")
    print(f"      Blend: {fold_acc*100:.2f}%")

# ============== LEVEL 2: STACKING META-MODEL ==============
print("\n[6/7] Training Level-2 Meta-Model...")

# Stack OOF predictions
oof_stack = np.column_stack([oof_lgb, oof_xgb, oof_ridge])
test_stack = np.column_stack([test_lgb, test_xgb, test_ridge])

# Train meta-model (Logistic Regression on sign prediction)
y_binary = (y > 0).astype(int)
meta_model = LogisticRegression(C=1.0, max_iter=1000, random_state=RANDOM_SEED)
meta_model.fit(oof_stack, y_binary)

final_oof_prob = meta_model.predict_proba(oof_stack)[:, 1]
final_test_prob = meta_model.predict_proba(test_stack)[:, 1]

# Also try simple weighted blend
blend_weights = [0.5, 0.35, 0.15]  # LGB, XGB, Ridge
simple_blend_oof = blend_weights[0] * oof_lgb + blend_weights[1] * oof_xgb + blend_weights[2] * oof_ridge
simple_blend_test = blend_weights[0] * test_lgb + blend_weights[1] * test_xgb + blend_weights[2] * test_ridge

# ============== FINAL RESULTS ==============
print("\n[7/7] Final Results...")

# Compare approaches
meta_acc = accuracy_score(y_binary, (final_oof_prob > 0.5).astype(int))
blend_acc = accuracy_score(y_binary, (simple_blend_oof > 0).astype(int))
lgb_acc = accuracy_score(y_binary, (oof_lgb > 0).astype(int))
xgb_acc = accuracy_score(y_binary, (oof_xgb > 0).astype(int))
ridge_acc = accuracy_score(y_binary, (oof_ridge > 0).astype(int))

print(f"\n{'=' * 70}")
print(f"   ğŸ“Š OOF ACCURACY COMPARISON:")
print(f"      LightGBM:    {lgb_acc * 100:.2f}%")
print(f"      XGBoost:     {xgb_acc * 100:.2f}%")
print(f"      Ridge:       {ridge_acc * 100:.2f}%")
print(f"      Simple Blend: {blend_acc * 100:.2f}%")
print(f"      Meta-Model:  {meta_acc * 100:.2f}%")
print(f"{'=' * 70}")
print(f"   ğŸ† BEST OOF: {max(meta_acc, blend_acc) * 100:.2f}%")
print(f"   ğŸ“Š Mean Fold: {np.mean(fold_scores) * 100:.2f}% (+/- {np.std(fold_scores) * 100:.2f}%)")
print(f"{'=' * 70}")

# Use best approach for submission
if meta_acc >= blend_acc:
    final_pred = (final_test_prob > 0.5).astype(int)
    print("   Using Meta-Model predictions")
else:
    final_pred = (simple_blend_test > 0).astype(int)
    print("   Using Simple Blend predictions")

# ============== SAVE SUBMISSION ==============
print("\n   Saving submission...")
submission = pd.DataFrame(final_pred, index=sample_submission.index, columns=['prediction'])
submission.to_csv(OUTPUT_PATH)

print(f"   ğŸ“ Saved: {OUTPUT_PATH}")
print(f"   ğŸ“ˆ Predictions: {len(submission):,}")
print(f"   Distribution: 1={final_pred.sum()} ({final_pred.mean()*100:.1f}%), 0={len(final_pred)-final_pred.sum()}")

print("\nğŸš€ V2 ULTIMATE COMPLETE! ğŸš€")

gc.collect()
